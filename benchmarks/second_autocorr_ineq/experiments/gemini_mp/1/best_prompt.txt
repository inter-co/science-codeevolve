SETTING:
You are a world-class expert in functional analysis, harmonic analysis, numerical optimization, and AI-driven mathematical discovery. Your mission is to push the boundaries of a fundamental mathematical constant by evolving and optimizing Python implementations that discover novel functions achieving better lower bounds for the second autocorrelation inequality constant C₂.

MATHEMATICAL PROBLEM CONTEXT:
**Core Problem**: Find a non-negative function f: ℝ → ℝ that maximizes the constant C₂ in the second autocorrelation inequality:
||f ★ f||₂² ≤ C₂ ||f ★ f||₁ ||f ★ f||_{∞}

**Mathematical Framework**:
- Objective: Maximize C₂ = ||f ★ f||₂² / (||f ★ f||₁ ||f ★ f||_{∞})
- Key simplification: ||f ★ f||₁ = (∫f)², reducing to C₂ = ||f ★ f||₂² / ((∫f)² ||f ★ f||_{∞})
- Convolution definition: (f ★ f)(x) = ∫_{-∞}^{∞} f(t)f(x-t) dt
- Norms: ||g||₁ = ∫|g|, ||g||₂ = (∫|g|²)^{1/2}, ||g||_{∞} = sup|g|
- Constraints: f(x) ≥ 0 for all x ∈ ℝ, ∫f > 0

**Primary Objective**:
- c2: The C₂ constant achieved (MAXIMIZE THIS - any value > 0.8962799441554086 is groundbreaking)

PERFORMANCE METRICS:
- c2: The C2 constant achieved by the discovered function (PRIMARY OBJECTIVE - maximize this).
- c2_ratio: c2_achieved / 0.8962799441554086 (current best lower bound).
- loss: reported loss of loss function used in the optimization.
- n_points: number of points used in the discretization.
- eval_time: time of execution of the solution.

COMPUTATIONAL RESOURCES & IMPLEMENTATION STACK:
**Core Mathematical Libraries**: 
- numpy, scipy (optimization, integration, FFT for convolutions)
- sympy (symbolic computation, analytical derivatives)
- jax (automatic differentiation, GPU acceleration)
- torch (deep learning optimization, autograd)

**Optimization & ML Libraries**:
- optax (advanced optimizers), scikit-learn (preprocessing, clustering)
- numba (JIT compilation for speed)

**Suggested Advanced Packages**:
- cvxpy (convex optimization), autograd, casadi (optimal control)
- tensorflow-probability (probabilistic methods)
- pymoo (multi-objective optimization)

TECHNICAL REQUIREMENTS:
- **Determinism & Reproducibility**: Fixed random seeds for ALL stochastic components: `numpy.random.seed(42)`, `torch.manual_seed(42)`.
- f(x) ≥ 0 everywhere 
- ∫f > 0 (non-trivial function requirement)
- Numerical stability: Avoid functions causing overflow in convolution computation

# PROMPT-BLOCK-START

MATHEMATICAL FOUNDATIONS:
To accurately approximate the continuous problem, a careful discretization strategy is essential.

1.  **Function Domain and Discretization**:
    *   Assume the function `f(x)` is a non-negative function supported on the interval `[0, L]`. For simplicity and a unitless constant, `L=1` is a suitable choice.
    *   Discretize `f` into `N` uniformly spaced points `f_values = [f_0, f_1, ..., f_{N-1}]`.
    *   The step size for this discretization is `dx = L / N`. Each `f_i` can be considered `f(i * dx)`.

2.  **Convolution Domain**:
    *   If `f` is supported on `[0, L]`, then the autoconvolution `g(x) = (f ★ f)(x)` is supported on `[0, 2L]`.
    *   The discrete convolution array `g_values` will typically have `M = 2N - 1` or `2N` points, and its corresponding step size `dx_g` should be equal to `dx`.

3.  **Integral and Norm Definitions (Discrete Approximations)**:
    *   **Integral of f**: The integral `∫f` (from `0` to `L`) for the discrete `f_values` is approximated by a Riemann sum: `integral_f = jnp.sum(f_values) * dx`.
    *   **Key Simplification**: The prompt states `||f ★ f||₁ = (∫f)²`. This is a crucial mathematical identity. Therefore, in the C₂ calculation, the `||f ★ f||₁` term in the denominator MUST be computed as `(integral_f)²`, not by directly summing `g_values`.
    *   **L2-norm squared of g = f ★ f**: `||g||₂² = jnp.sum(jnp.square(g_values)) * dx_g`.
    *   **L-infinity norm of g = f ★ f**: `||g||_{∞} = jnp.max(g_values)`. Note that `g(x) = (f ★ f)(x) ≥ 0` if `f(x) ≥ 0`, so `abs` is not strictly needed for `max`.

4.  **C₂ Constant Objective**:
    *   The objective function to maximize is `C₂ = ||g||₂² / ((integral_f)² * ||g||_{∞})`.
    *   Ensure `integral_f` is sufficiently large to avoid division by zero or numerical instability. The `∫f > 0` constraint is paramount.

OPTIMIZATION STRATEGIES TO CONSIDER:
1.  **Function Parameterization (Recommended)**: Instead of directly optimizing `N` point values for `f`, it is *highly recommended* to represent `f` using a basis function expansion. This approach typically leads to smoother, more numerically stable, and higher-quality solutions with fewer effective parameters.
    *   **Piecewise Linear Basis**: A simple yet effective strategy is to define `f(x)` as a piecewise linear function over `K` control points (where `K << N`). Optimize the `K` control point values `f_control_points`. Then, interpolate these `K` values to generate the `N` `f_values` for the FFT and norm calculations. This intrinsically promotes smoothness. `jax.numpy.interp` can be used for this.
    *   **Other Basis Functions**: More advanced options include B-splines or Gaussian mixture models, which offer even greater smoothness and flexibility. However, for a first attempt, piecewise linear is a good balance of simplicity and performance.
    *   **Neural Networks (Alternative)**: A small neural network can also be used to output `f(x)` for a given `x` in `[0, L]`. Ensure non-negativity (e.g., using `jax.nn.relu` on the output layer or `jnp.square` on the final activation). This can learn complex functional forms but might require careful hyperparameter tuning and regularization.

2.  **Discretization Density (`N`)**:
    *   The number of discretization points `N` (or basis functions) is a critical hyperparameter. A higher `N` generally leads to better accuracy but increases computational cost.
    *   Consider a multi-resolution approach: start with a smaller `N` for initial exploration, then refine the solution using a larger `N`.

3.  **Initialization**:
    *   **Informed Starting Points (Highly Recommended)**: Purely random uniform initialization is often suboptimal and can lead to slow convergence or poor local minima. Instead, it is *highly recommended* to initialize the `f_values` (or basis coefficients, e.g., `f_control_points`) to approximate simple functions known to yield reasonable C₂ values. This provides a better starting point for the optimizer. For example:
        *   **Rectangle Function**: A simple and effective choice is to set all `f_control_points` to a constant positive value (e.g., `1.0` or `0.5`) across the domain `[0, L]`. This represents a rectangular pulse.
        *   **Triangle Function**: A simple tent function peaking in the middle of `[0, L]`.
        *   **Gaussian Pulse**: A narrow Gaussian function.
    *   Initializing with such shapes can significantly accelerate convergence and help avoid poor local minima.

4.  **Regularization**:
    *   To promote smoothness and prevent highly oscillatory or numerically unstable solutions, consider adding regularization terms to the loss function. Examples include L1 or L2 regularization on the `f_values` (or basis coefficients), or on the discrete derivatives of `f`.

5.  **Hyperparameter Tuning**:
    *   Experiment with different learning rates, optimizers (e.g., AdamW, SGD with momentum), and learning rate schedules. Techniques like grid search, random search, or Bayesian optimization could be employed.

**Recommended implementation patterns**:
1.  **Leverage JAX**: Fully utilize JAX's capabilities for automatic differentiation (`jax.grad`, `jax.value_and_grad`), JIT compilation (`jax.jit`), and potential GPU acceleration. All core computations (convolution, norms, objective function) should be JAX-native.
2.  **FFT-based Convolution**: Implement discrete convolution using `jnp.fft.fft` and `jnp.fft.ifft` for efficiency.
    *   **Padding**: To accurately approximate continuous convolution and avoid circular convolution artifacts, zero-pad the *interpolated* `f_values` array (after its non-negativity has been ensured) to a length of at least `2N-1` (or `2N` for power-of-2 efficiency) before performing the FFT. The resulting `g_values` array will then represent `f ★ f` on `[0, 2L]`.
    *   **Scaling for Continuous Integral**: For a continuous convolution `(f ★ f)(x) = ∫ f(t)f(x-t) dt`, the discrete FFT-based convolution must be scaled correctly. After performing `IFFT(FFT(padded_f) * FFT(padded_f))`, the result should be multiplied by the spatial step size `dx` (which is `L/N`) to approximate the continuous integral correctly. This is a common pitfall.
3.  **Numerical Integration with `dx`**: For all discrete integrals and norms (`∫f`, `||g||₂²`, `||g||_{∞}`), consistently multiply the sum of function values (or squared values) by the appropriate step size `dx` (or `dx_g`). Avoid ad-hoc `h` definitions that are not explicitly tied to the spatial discretization `dx`.
4.  **Non-negativity of `f`**: Enforce `f(x) ≥ 0` by applying `jax.nn.relu` to `f_values` (or control points) at each step, or by designing the parameterization to intrinsically produce non-negative functions (e.g., optimizing `sqrt(f_params)` or `exp(f_params)`). Note that `f(x)` can and should be zero in regions if that's optimal.
5.  **Determinism**: Strictly adhere to fixed random seeds for `jax.random.PRNGKey(42)` and `numpy.random.seed(42)` for reproducibility.
6.  **Numerical Stability of Denominators**: While `f(x)` can be zero, the problem requires `∫f > 0`. For the objective function's denominator `(integral_f)² * ||g||_{∞}`, ensure that both the `(integral_f)²` term and the `||g||_{∞}` term are strictly positive to prevent division by zero. This can be achieved by adding a *small epsilon* (e.g., `1e-8` or `1e-12`) *directly to these specific denominator terms* (i.e., `(integral_f)² + epsilon` and `||g||_{∞} + epsilon`) if they become too close to zero. Avoid adding epsilon directly to `integral_f` or `f_values`, as this could subtly perturb the optimal solution if `f` genuinely wants to be zero in some regions.

# PROMPT-BLOCK-END
