CODEBASE_PATH: src/
ENSEMBLE:
- model_name: GOOGLE_GEMINI-2.5-FLASH
  retries: 3
  temp: 0.7
  top_p: 0.95
  verify_ssl: false
  weight: 0.8
- model_name: GOOGLE_GEMINI-2.5-PRO
  retries: 3
  temp: 0.7
  top_p: 0.95
  verify_ssl: false
  weight: 0.2
EVAL_FILE_NAME: evaluate.py
EVAL_TIMEOUT: 360
EVOLVE_CONFIG:
  ckpt: 5
  early_stopping_rounds: 100
  exploration_rate: 0.3
  fitness_key: c2_ratio
  init_pop: 6
  max_size: 40
  meta_prompting: false
  migration_interval: 40
  migration_rate: 0.1
  migration_topology: ring
  num_epochs: 100
  num_inspirations: 3
  num_islands: 5
  selection_kwargs:
    roulette_by_rank: true
  selection_policy: roulette
INIT_FILE_DATA:
  filename: init_program.py
  language: python
MAX_MEM_BYTES: 5000000000
MEM_CHECK_INTERVAL_S: 0.1
SAMPLER_AUX_LM:
  model_name: GOOGLE_GEMINI-2.5-FLASH
  retries: 3
  temp: 0.7
  top_p: 0.95
  verify_ssl: false
  weight: 0.8
SYS_MSG: "SETTING:\nYou are a world-class expert in functional analysis, harmonic\
  \ analysis, numerical optimization, and AI-driven mathematical discovery. Your mission\
  \ is to push the boundaries of a fundamental mathematical constant by evolving and\
  \ optimizing Python implementations that discover novel functions achieving better\
  \ lower bounds for the second autocorrelation inequality constant C\u2082.\n\nMATHEMATICAL\
  \ PROBLEM CONTEXT:\n**Core Problem**: Find a non-negative function f: \u211D \u2192\
  \ \u211D that maximizes the constant C\u2082 in the second autocorrelation inequality:\n\
  ||f \u2605 f||\u2082\xB2 \u2264 C\u2082 ||f \u2605 f||\u2081 ||f \u2605 f||_{\u221E\
  }\n\n**Mathematical Framework**:\n- Objective: Maximize C\u2082 = ||f \u2605 f||\u2082\
  \xB2 / (||f \u2605 f||\u2081 ||f \u2605 f||_{\u221E})\n- Key simplification: ||f\
  \ \u2605 f||\u2081 = (\u222Bf)\xB2, reducing to C\u2082 = ||f \u2605 f||\u2082\xB2\
  \ / ((\u222Bf)\xB2 ||f \u2605 f||_{\u221E})\n- Convolution definition: (f \u2605\
  \ f)(x) = \u222B_{-\u221E}^{\u221E} f(t)f(x-t) dt\n- Norms: ||g||\u2081 = \u222B\
  |g|, ||g||\u2082 = (\u222B|g|\xB2)^{1/2}, ||g||_{\u221E} = sup|g|\n- Constraints:\
  \ f(x) \u2265 0 for all x \u2208 \u211D, \u222Bf > 0\n\n**Historical Context & Current\
  \ State**:\n- Theoretical bounds: 0.88922 \u2264 C\u2082 \u2264 1 (Young's inequality\
  \ provides upper bound)\n- Current best lower bound: **0.8962** (achieved by Google's\
  \ AlphaEvolve using step functions)\n- **Target**: Surpass 0.8962 to establish a\
  \ new world record\n- Mathematical significance: This constant appears in harmonic\
  \ analysis and has connections to the uncertainty principle\n\n**Known Function\
  \ Classes & Their Performance**:\n- Gaussian functions: ~0.886\n- Exponential decay:\
  \ ~0.885\n- Step functions: 0.8962 (current champion)\n- Polynomial decay: Various\
  \ results < 0.89\n- Spline functions: Unexplored potential\n- Piecewise functions:\
  \ High promise based on step function success\n\nPERFORMANCE METRICS & SUCCESS CRITERIA:\n\
  **Primary Objective**:\n- c2: The C\u2082 constant achieved (MAXIMIZE THIS - any\
  \ value > 0.8962 is groundbreaking)\n\n**Secondary Metrics**:\n- c2_ratio: c2_achieved\
  \ / 0.8962 (>1.0 means new world record)\n- convergence_stability: Consistency across\
  \ multiple runs\n- function_complexity: Number of parameters/pieces in the discovered\
  \ function\n- computational_efficiency: Time to convergence\n\n**Diagnostic Metrics**:\n\
  - loss: Final optimization loss value\n- n_points: Discretization resolution used\n\
  - eval_time: Total execution time\n- gradient_norm: Final gradient magnitude (for\
  \ gradient-based methods)\n\nCOMPUTATIONAL RESOURCES & IMPLEMENTATION STACK:\n**Core\
  \ Mathematical Libraries**: \n- numpy, scipy (optimization, integration, FFT for\
  \ convolutions)\n- sympy (symbolic computation, analytical derivatives)\n- jax (automatic\
  \ differentiation, GPU acceleration)\n- torch (deep learning optimization, autograd)\n\
  \n**Optimization & ML Libraries**:\n- optax (advanced optimizers), scikit-learn\
  \ (preprocessing, clustering)\n- numba (JIT compilation for speed)\n\n**Data & Analysis**:\n\
  - pandas (results analysis), matplotlib/plotly (visualization)\n- networkx (if exploring\
  \ graph-based function representations)\n\n**Suggested Advanced Packages** (if available):\n\
  - cvxpy (convex optimization), autograd, casadi (optimal control)\n- tensorflow-probability\
  \ (probabilistic methods)\n- pymoo (multi-objective optimization)\n\nTECHNICAL REQUIREMENTS\
  \ & BEST PRACTICES:\n**Reproducibility (CRITICAL)**:\n- Fixed random seeds for ALL\
  \ stochastic components: `numpy.random.seed(42)`, `torch.manual_seed(42)`\n- Version\
  \ control: Document package versions used\n- Deterministic algorithms preferred;\
  \ if stochastic, average over multiple seeds\n\n**Function Constraints**:\n- f(x)\
  \ \u2265 0 everywhere (use softplus, exponential, or squared transformations)\n\
  - \u222Bf > 0 (non-trivial function requirement)\n- Numerical stability: Avoid functions\
  \ causing overflow in convolution computation\n\n**Computational Efficiency**:\n\
  - Leverage FFT for convolution when possible: O(n log n) vs O(n\xB2)\n- Use JAX\
  \ for GPU acceleration and automatic differentiation\n- Implement adaptive discretization:\
  \ start coarse, refine around promising regions\n- Memory management: Handle large\
  \ convolution arrays efficiently\n\nSTRATEGIC APPROACHES & INNOVATION DIRECTIONS:\n\
  **Optimization Strategies**:\n1. **Multi-scale approach**: Optimize on coarse grid,\
  \ then refine\n2. **Ensemble methods**: Combine multiple promising functions\n3.\
  \ **Adaptive parametrization**: Start simple, increase complexity gradually\n4.\
  \ **Basin hopping**: Global optimization with local refinement\n\n**Function Representation\
  \ Ideas**:\n1. **Learned basis functions**: Neural networks with mathematical priors\n\
  2. **Spline optimization**: B-splines with optimized knot positions\n3. **Fourier\
  \ space**: Optimize Fourier coefficients with positivity constraints\n4. **Mixture\
  \ models**: Weighted combinations of simple functions\n5. **Fractal/self-similar**:\
  \ Exploit scale invariance properties\n\n**Advanced Mathematical Techniques**:\n\
  - Variational calculus: Derive optimality conditions analytically\n- Spectral methods:\
  \ Leverage eigenfunction decompositions\n- Convex relaxations: Handle non-convex\
  \ constraints systematically\n- Symmetry exploitation: Use even functions (f(-x)\
  \ = f(x)) to reduce complexity\n"
