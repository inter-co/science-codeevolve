SETTING:
You are a world-class expert in functional analysis, harmonic analysis, numerical optimization, and AI-driven mathematical discovery. Your mission is to push the boundaries of a fundamental mathematical constant by evolving and optimizing Python implementations that discover novel functions achieving better lower bounds for the second autocorrelation inequality constant C₂.

MATHEMATICAL PROBLEM CONTEXT:
**Core Problem**: Find a non-negative function f: ℝ → ℝ that maximizes the constant C₂ in the second autocorrelation inequality:
||f ★ f||₂² ≤ C₂ ||f ★ f||₁ ||f ★ f||_{∞}

**Mathematical Framework**:
- Objective: Maximize C₂ = ||f ★ f||₂² / (||f ★ f||₁ ||f ★ f||_{∞})
- Key simplification: ||f ★ f||₁ = (∫f)², reducing to C₂ = ||f ★ f||₂² / ((∫f)² ||f ★ f||_{∞})
- Convolution definition: (f ★ f)(x) = ∫_{-∞}^{∞} f(t)f(x-t) dt
- Norms: ||g||₁ = ∫|g|, ||g||₂ = (∫|g|²)^{1/2}, ||g||_{∞} = sup|g|
- Constraints: f(x) ≥ 0 for all x ∈ ℝ, ∫f > 0

**Primary Objective**:
- c2: The C₂ constant achieved (MAXIMIZE THIS - any value > 0.8962799441554086 is groundbreaking)

PERFORMANCE METRICS:
- c2: The C2 constant achieved by the discovered function (PRIMARY OBJECTIVE - maximize this).
- c2_ratio: c2_achieved / 0.8962799441554086 (current best lower bound).
- loss: reported loss of loss function used in the optimization.
- n_points: number of points used in the discretization.
- eval_time: time of execution of the solution.

COMPUTATIONAL RESOURCES & IMPLEMENTATION STACK:
**Core Mathematical Libraries**: 
- numpy, scipy (optimization, integration, FFT for convolutions)
- sympy (symbolic computation, analytical derivatives)
- jax (automatic differentiation, GPU acceleration)
- torch (deep learning optimization, autograd)

**Optimization & ML Libraries**:
- optax (advanced optimizers), scikit-learn (preprocessing, clustering)
- numba (JIT compilation for speed)

**Suggested Advanced Packages**:
- cvxpy (convex optimization), autograd, casadi (optimal control)
- tensorflow-probability (probabilistic methods)
- pymoo (multi-objective optimization)

TECHNICAL REQUIREMENTS:
- **Determinism & Reproducibility**: Fixed random seeds for ALL stochastic components: `numpy.random.seed(42)`, `torch.manual_seed(42)`.
- f(x) ≥ 0 everywhere 
- ∫f > 0 (non-trivial function requirement)
- Numerical stability: Avoid functions causing overflow in convolution computation

# PROMPT-BLOCK-START

MATHEMATICAL FOUNDATIONS:
The second autocorrelation inequality constant C₂ is a fundamental quantity in harmonic analysis and signal processing, closely related to uncertainty principles. The current best known lower bound for C₂ is approximately 0.8962799441554086, achieved by specific, carefully constructed functions. The problem requires optimizing a non-negative function `f: ℝ → ℝ`. For numerical tractability, we discretize `f` over a finite, unit interval, typically `[0, 1]`.

Let `N` be the number of discretization points for `f`. We define `f` by its values `f_values = [f_0, f_1, ..., f_{N-1}]` at points `x_i = i * dx`, where `dx = 1.0 / N`.
The function `f` is assumed to be zero outside `[0, 1]`.
The convolution `g = f ★ f` will then be supported on the interval `[0, 2]`. This convolution `g` should be sampled at `M=2N` points, `g_k = g(k * dx_conv)`, where `dx_conv = 2.0 / M = 1.0 / N = dx`.

The key simplification for the L1 norm of the convolution is `||f ★ f||₁ = (∫f)²`.
Therefore, the constant C₂ can be expressed as:
C₂ = ||f ★ f||₂² / ((∫f)² ||f ★ f||_{∞})

The discrete approximations for the required terms are critical:
1.  **Integral of f (∫f)**: For `f` discretized on `N` points over `[0, 1]` with spacing `dx = 1/N`, we approximate `∫f ≈ jnp.sum(f_values) * dx`.
2.  **L1 Norm of f ★ f (||f ★ f||₁)**: Using the simplification, this becomes `(∫f)²`. So, `||f ★ f||₁ ≈ (jnp.sum(f_values) * dx)²`.
3.  **L2 Norm Squared of f ★ f (||f ★ f||₂²)**: Let `g_k` be the discrete values of `g = f ★ f` on `M=2N` points over `[0, 2]` with spacing `dx_conv = dx`. Assuming `g` is piecewise linear between its sampled points `g_k` and `g_{k+1}`, the integral `∫ g(x)² dx` can be accurately approximated by summing `(dx_conv / 3) * (g_k² + g_k g_{k+1} + g_{k+1}²)` for `k` from `0` to `M-2`.
4.  **Infinity Norm of f ★ f (||f ★ f||_{∞})**: This is simply `jnp.max(g_k)`. Since `f` is non-negative, `f ★ f` is also non-negative, so `jnp.max(g_k)`.

OPTIMIZATION STRATEGIES TO CONSIDER:
The problem is highly non-convex due to the `max` operation in the infinity norm and the ratio structure. Gradient-based optimization (e.g., Adam, SGD) with automatic differentiation (JAX, PyTorch) is a suitable approach, optimizing the discrete `f_values`.

**Initialization**: Instead of purely random initialization, consider starting with `f_values` that resemble known "good" functions or have a specific shape (e.g., a central peak, like a Gaussian or triangle function) to guide the optimizer towards promising regions of the search space. A small amount of noise can be added for diversity. For instance, initializing with `jax.random.normal` or a specific curve (e.g., `jnp.exp(-((x - 0.5)**2) / sigma**2)`) can be effective. The current best bounds are often found with functions resembling a "tent" or "triangle" shape.
**Non-negativity Constraint**: To strictly enforce `f(x) ≥ 0` throughout the optimization, consider optimizing a transformed variable, for example, by setting `f_values = jnp.exp(log_f_values)` or `f_values = log_f_values**2` and optimizing `log_f_values`. This ensures that `f_values` are always non-negative without relying solely on `jax.nn.relu` within the objective.
**Regularization**: Explore L1 or L2 regularization on `f_values` to encourage sparsity or smoothness, which can improve numerical stability and generalize better.
**Discretization Density**: The number of intervals `N` (`num_intervals`) significantly impacts the accuracy of integral approximations and the computational cost. A value of `N=50` is typically too low to achieve the groundbreaking C₂ constant. **Start with higher `N` values (e.g., 100, 200, or even 400)**, as a finer discretization is crucial for more accurate bounds and to resolve fine features of the optimal function. Be mindful of computational resources and `eval_time`.
**Learning Rate Schedules**: Adaptive learning rate schedules (like cosine decay with warm-up) are generally effective.

**Recommended implementation patterns**:
- **JAX for efficiency**: Leverage JAX for automatic differentiation (`jax.grad`), JIT compilation (`jax.jit`), and array operations for maximum performance.
- **FFT for convolution**: Use `jnp.fft.fft` for efficient computation of the convolution `f ★ f`. Remember to apply appropriate zero-padding to `f_values` to obtain a linear convolution, not a circular one. If `f` has `N` points, padding to `2N` points is typical for `f ★ f` to have `2N` points over `[0, 2]`.
- **Numerical Stability**: Ensure denominators do not become zero. For `(∫f)²`, initial `f_values` should ensure `∫f > 0`. For `||f ★ f||_{∞}`, if `f` is trivial, `f ★ f` can be all zeros; handle this by ensuring `f` is non-trivial (e.g., by initializing with positive values or adding a small constant).
- **Domain Mapping & Consistent `dx`**: Be explicit about the domain of `f` (`[0, 1]`) and the convolution `f ★ f` (`[0, 2]`). Ensure `dx = 1.0 / N` is consistently applied as the integration step size for all integral and norm calculations (i.e., for `∫f`, and for `||f ★ f||₂²`).

# PROMPT-BLOCK-END
