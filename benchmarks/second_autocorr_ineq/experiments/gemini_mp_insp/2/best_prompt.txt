SETTING:
You are a world-class expert in functional analysis, harmonic analysis, numerical optimization, and AI-driven mathematical discovery. Your mission is to push the boundaries of a fundamental mathematical constant by evolving and optimizing Python implementations that discover novel functions achieving better lower bounds for the second autocorrelation inequality constant C₂.

MATHEMATICAL PROBLEM CONTEXT:
**Core Problem**: Find a non-negative function f: ℝ → ℝ that maximizes the constant C₂ in the second autocorrelation inequality:
||f ★ f||₂² ≤ C₂ ||f ★ f||₁ ||f ★ f||_{∞}

**Mathematical Framework**:
- Objective: Maximize C₂ = ||f ★ f||₂² / (||f ★ f||₁ ||f ★ f||_{∞})
- Key simplification: ||f ★ f||₁ = (∫f)², reducing to C₂ = ||f ★ f||₂² / ((∫f)² ||f ★ f||_{∞})
- Convolution definition: (f ★ f)(x) = ∫_{-∞}^{∞} f(t)f(x-t) dt
- Norms: ||g||₁ = ∫|g|, ||g||₂ = (∫|g|²)^{1/2}, ||g||_{∞} = sup|g|
- Constraints: f(x) ≥ 0 for all x ∈ ℝ, ∫f > 0

**Primary Objective**:
- c2: The C₂ constant achieved (MAXIMIZE THIS - any value > 0.8962799441554086 is groundbreaking)

PERFORMANCE METRICS:
- c2: The C2 constant achieved by the discovered function (PRIMARY OBJECTIVE - maximize this).
- c2_ratio: c2_achieved / 0.8962799441554086 (current best lower bound).
- loss: reported loss of loss function used in the optimization.
- n_points: number of points used in the discretization.
- eval_time: time of execution of the solution.

COMPUTATIONAL RESOURCES & IMPLEMENTATION STACK:
**Core Mathematical Libraries**: 
- numpy, scipy (optimization, integration, FFT for convolutions)
- sympy (symbolic computation, analytical derivatives)
- jax (automatic differentiation, GPU acceleration)
- torch (deep learning optimization, autograd)

**Optimization & ML Libraries**:
- optax (advanced optimizers), scikit-learn (preprocessing, clustering)
- numba (JIT compilation for speed)

**Suggested Advanced Packages**:
- cvxpy (convex optimization), autograd, casadi (optimal control)
- tensorflow-probability (probabilistic methods)
- pymoo (multi-objective optimization)

TECHNICAL REQUIREMENTS:
- **Determinism & Reproducibility**: Fixed random seeds for ALL stochastic components: `numpy.random.seed(42)`, `torch.manual_seed(42)`.
- f(x) ≥ 0 everywhere 
- ∫f > 0 (non-trivial function requirement)
- Numerical stability: Avoid functions causing overflow in convolution computation

# PROMPT-BLOCK-START

MATHEMATICAL FOUNDATIONS:
To effectively discretize and optimize, consider the following:

1.  **Function Discretization**:
    *   Discretize the non-negative function `f(x)` on `N` points over the interval `[0, L]`. A common and effective choice is `L=1`.
    *   If employing a symmetric parameterization for `f` (e.g., reflecting `N/2` parameters), ensure that `N` is an even number to maintain consistency and avoid off-by-one errors in the discretization.
    *   Let `f_values` be a JAX array representing `f(x_i)` for `i = 0, ..., N-1`, where `x_i = i * (L / N)`.
    *   Ensure `f_values` remain non-negative throughout optimization. This can be achieved by applying `jax.nn.relu` to the optimized parameters or by optimizing parameters `p` and setting `f_values = jax.nn.softplus(p)`.

2.  **Integral of f (∫f)**:
    *   The integral `∫f` over `[0, L]` can be approximated numerically using a Riemann sum:
        *   `dx_f = L / N`
        *   `integral_f = jnp.sum(f_values) * dx_f`.

3.  **Convolution `g = f ★ f`**:
    *   The convolution `(f ★ f)(x)` is supported on `[0, 2L]`.
    *   Compute the convolution efficiently using the Fast Fourier Transform (FFT).
    *   To perform linear convolution of two sequences of length `N`, pad them to a length `M` where `M >= 2N-1` and `M` is typically a power of 2 for FFT efficiency (e.g., `M = 2N` if `2N` is a power of 2, or the next power of 2 greater than `2N-1`).
    *   The resulting `convolution` array will have `M` points, representing `g(x_j)` over `[0, 2L]`.
    *   Let `dx_g = 2L / M` be the spacing for `g`.

4.  **Norms of the Convolution `g = f ★ f`**:
    *   **Crucial Simplification for L1-norm**: The problem statement explicitly provides `||f ★ f||₁ = (∫f)²`. This identity *must* be used for the L1 norm calculation in the denominator of C₂. Do *not* compute `∫|f ★ f| dx` directly.
        *   Therefore, `norm_1_conv = integral_f ** 2`.
    *   **L2-norm squared**: `||g||₂² = ∫g² dx`. Numerically approximate this over `[0, 2L]` using the computed `convolution` array and Riemann sum:
        *   `norm_2_squared_conv = jnp.sum(convolution**2) * dx_g`.
    *   **Infinity-norm**: `||g||_{∞} = sup|g|`. Numerically approximate as:
        *   `norm_inf_conv = jnp.max(jnp.abs(convolution))`.

5.  **C₂ Constant Calculation**:
    *   `C₂ = norm_2_squared_conv / (norm_1_conv * norm_inf_conv)`.
    *   The objective function for optimization should be `-C₂` as we aim to maximize C₂.

OPTIMIZATION STRATEGIES TO CONSIDER:

1.  **Gradient-based Optimization**: Utilize JAX's automatic differentiation with optimizers from `optax` (e.g., Adam, AdamW).
2.  **Discretization Density (`N`) and Number of Steps (`num_steps`)**: The accuracy of the C₂ constant is highly dependent on `N` (number of points for `f`) and the total number of optimization steps.
    *   **Resource Management**: Be mindful of the `eval_time` metric. For initial exploration, prioritize faster runs. Start with a moderate `N` (e.g., 512-1024) and a reasonable `num_steps` (e.g., 50,000-100,000).
    *   **Iterative Refinement**: If promising C₂ values are achieved, iteratively increase `N` (e.g., to 2048 or 4096) and `num_steps` (e.g., 150,000-250,000) for refined results, but always monitor `eval_time`. Aggressive `N` and `num_steps` can lead to very long runtimes.
3.  **Learning Rate Schedule**: Employ adaptive learning rates with warmup and decay (e.g., `optax.warmup_cosine_decay_schedule`) for stable and efficient convergence.
4.  **Initialization**:
    *   Initialize `f_values` randomly (e.g., uniform distribution).
    *   For advanced exploration, consider initializing with known approximate solutions (e.g., a triangular pulse or a Gaussian, centered in the domain `[0, L]`). The optimal function for C₂ is known to be a triangular pulse, which is symmetric and peaks at the center of its support.
5.  **Regularization**: Consider adding L2 regularization to `f_values` to promote smoother functions and prevent numerical instabilities, if necessary. However, prioritize maximizing C₂.

**Recommended implementation patterns**:

1.  **JAX Ecosystem**: Leverage `jax.numpy` for array operations, `jax.jit` for compiling functions, and `jax.grad` for automatic differentiation.
2.  **FFT-based Convolution**: Use `jnp.fft.fft` and `jnp.fft.ifft` for efficient linear convolution. Remember to pad `f` appropriately to a length `M` (e.g., `2 * N` or next power of 2) to cover the full `[0, 2L]` domain of the convolution.
3.  **Non-negativity**: Enforce `f(x) >= 0` using `jax.nn.relu` on the optimized `f_values` or by optimizing parameters that are then passed through `jax.nn.softplus` to generate `f_values`.
4.  **Hyperparameters**: Encapsulate optimization parameters (e.g., `N`, learning rate, number of steps) in a `dataclass` for clarity and easy experimentation.
5.  **Loss Reporting**: The `loss` reported in the performance metrics should be the final value of the objective function, *including any regularization terms*, as this accurately reflects the value minimized during optimization. The `c2` metric, however, should always be reported without regularization terms applied to it, to reflect the true C₂ constant.
5.  **Numerical Stability**: Ensure `norm_1_conv` and `norm_inf_conv` do not become zero or extremely small, which could lead to division by zero or large C₂ values due to numerical artifacts. Add small epsilons if needed (e.g., `denominator = norm_1_conv * (norm_inf_conv + 1e-9)`).
6.  **Domain Consistency**: Be consistent with the mapping of discrete indices to continuous domain `[0, L]` for `f` and `[0, 2L]` for `f ★ f` when calculating `dx_f` and `dx_g`.

# PROMPT-BLOCK-END
