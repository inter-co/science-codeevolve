SETTING:
You are an expert computational geometer and optimization specialist focusing on point dispersion problems.
Your task is to evolve a constructor function that generates an optimal arrangement of exactly 16 points in 2D space, maximizing the ratio of minimum distance to maximum distance between all point pairs.

PROBLEM CONTEXT:
- Target: Beat the AlphaEvolve benchmark of min/max ratio = 1/√12.889266112 ≈ 0.2786
- Constraint: Points must be placed in 2D Euclidean space (typically normalized to unit square [0,1] × [0,1])
- Mathematical formulation: For points Pi = (xi, yi), i = 1,...,16:
  * Distance matrix: dij = √[(xi-xj)² + (yi-yj)²] for all i≠j
  * Minimum distance: dmin = min{dij : i≠j}
  * Maximum distance: dmax = max{dij : i≠j}
  * Objective: maximize dmin/dmax subject to spatial constraints

PERFORMANCE METRICS:
1. **min_max_ratio**: dmin/dmax ratio (PRIMARY OBJECTIVE - maximize)
2. **benchmark_ratio**: min_max_ratio / 0.2786 (progress toward beating AlphaEvolve benchmark)
3. **eval_time**: Execution time in seconds (balance accuracy vs. efficiency)

COMPUTATIONAL RESOURCES:
**Core packages**: numpy, scipy, sympy, pandas
**Additional useful packages**:
- **3D optimization**: `scipy.optimize`, `deap`, `platypus`, `pymoo` (multi-objective)
- **3D geometric computing**: 
  * `scipy.spatial` (3D distance matrices, ConvexHull, SphericalVoronoi)
  * `trimesh` (3D mesh operations), `open3d` (3D data processing)
- **Specialized 3D algorithms**: 
  * `spherical-geometry` for spherical arrangements
  * `quaternion` package for 3D rotations
- **Performance**: `numba` (3D JIT compilation), `joblib`

TECHNICAL REQUIREMENTS:
- **Reproducibility**: Fixed random seeds for all stochastic components

# PROMPT-BLOCK-START

OPTIMIZATION STRATEGIES TO CONSIDER:
The primary goal is to maximize the min/max ratio. However, it is paramount to achieve this within a practical `eval_time`. Solutions taking hundreds of seconds are too slow. Focus on optimization strategies that efficiently balance robust global exploration with precise local exploitation.
- **Symmetry exploitation**: Leverage rotational and reflectional symmetries of optimal configurations
- **Regular polygon foundations**: Start with vertices of regular polygons and perturb systematically
- **Multi-scale approaches**: 
  * Coarse grid initialization followed by continuous optimization
  * Hierarchical placement (outer boundary points first, then interior)
- **Adaptive algorithms (for global exploration, balancing speed and coverage)**: 
  * **Differential Evolution (DE)**: A powerful global optimizer. However, be mindful that *excessive* `maxiter`, `popsize`, or `num_restarts` *will* lead to very long `eval_time`. For this problem, `eval_time` is a strict constraint; therefore, prioritize *lower* values for these parameters in the initial global search phases to ensure feasibility within the time limit. Tune them to achieve *good enough* results quickly, rather than aiming for absolute perfection in the global stage.
  * **Covariance Matrix Adaptation Evolution Strategy (CMA-ES)**: Often more efficient than DE for continuous, non-convex problems with many variables (like 32 here). It adaptively learns the search distribution, potentially converging faster. When using CMA-ES, **avoid setting an overly optimistic `ftarget`**, as this can lead to excessive function evaluations trying to reach an unreachable or hard-to-find optimum. It's often better to rely on a fixed `maxfevals` for the global search phase.
  * Simulated annealing with adaptive temperature schedules
  * Particle swarm optimization with velocity clamping
- **Gradient-free methods (for local refinement or when gradients are unavailable/unreliable)**: 
  * Nelder-Mead simplex for robust local search
  * Powell's method for coordinate descent
  * COBYLA for constrained optimization without gradients (useful for final polish on non-smooth objectives)
- **Hybrid optimization (essential for combining global search with efficient local refinement)**:
  * **Basin-hopping**: An effective hybrid strategy that combines global stochastic jumps with local minimization. It can be more efficient than manual multi-start DE.
  * Memetic algorithms combining global and local search
  * Multiple restart strategies: If used, prioritize diverse initializations (e.g., quasi-random sequences, symmetric patterns) over simply increasing the number of restarts, and limit iterations per restart.
  * Genetic algorithms with local refinement operators (ensure local refinement is efficient)
- **Geometric heuristics**:
  * Maximin designs from experimental design theory
  * Voronoi-based adaptive placement
  * Force-directed layouts with repulsive interactions
  
GEOMETRIC INSIGHTS & MATHEMATICAL FOUNDATIONS:
- **Packing vs. dispersion duality**: This problem is related to but distinct from circle packing - focuses on point placement rather than area optimization
- **Scale invariance**: Optimal ratio is independent of coordinate system scaling
- **Boundary effects**: Unlike infinite plane problems, finite domains create edge effects that influence optimal configurations
- **Critical configurations**: 
  * Regular polygon (16-gon): All points on circle boundary
  * Square grid: 4×4 arrangement with uniform spacing
  * Hexagonal approximations: Triangular lattice subsets
- **Theoretical bounds**:
  * Upper bound: Perfect regular 16-gon gives ratio = cos(π/16)/√2 ≈ 0.383
  * Lower bound: Random placement typically yields ratios < 0.1
  * AlphaEvolve benchmark: 0.2786 represents significant progress toward theoretical limits
- **Symmetry groups**: Optimal solutions likely respect dihedral symmetries D16, D8, D4, or D2
- **Local vs. global optima**: High-dimensional landscape with many local optima requires sophisticated global optimization

IMPLEMENTATION GUIDELINES:
**Recommended implementation patterns (critical for balancing `min_max_ratio` with `eval_time`):**
- **Distance computation**: 
  * `scipy.spatial.distance.pdist()` for efficient pairwise distances. For custom, performance-critical loops or array operations in the objective function, consider using `numba`'s JIT compilation.
  * Vectorized operations using broadcasting for gradient computation.
- **Optimization frameworks**:
  * `scipy.optimize.minimize()` with method='L-BFGS-B' (or 'SLSQP', 'TNC') for gradient-based local refinement. When employing smoothing techniques (e.g., `k`-continuation with LogSumExp), it is **critical to strictly limit both the number of `k` steps and the `maxiter` for each local optimization** to manage `eval_time`. A sequence of **3-5 `k` values** is typically sufficient, and `maxiter` for each step should be conservative (e.g., **200-500 iterations**), as the goal is to quickly move closer to the optimum, not fully converge at each `k` step.
  * `scipy.optimize.differential_evolution()` for global search: **Aggressively tune `maxiter`, `popsize`, and `num_restarts` by *reducing* them significantly to meet `eval_time` targets.** High values *will* lead to excessively long runtimes. For initial global exploration, prioritize speed over ultimate precision.
  * `scipy.optimize.basinhopping()` or `scipy.optimize.shgo()` are highly recommended as robust **hybrid global optimizers** that can often be more efficient than manually chaining separate global and local methods. They internally manage the balance between exploration and exploitation.
  * **CMA-ES (e.g., via `cma` package if available)**: This is a strong choice for initial global search. When using CMA-ES, set a strict `maxfevals` limit (e.g., **20,000-50,000, not 100,000+**) to control `eval_time`. **Avoid setting an overly ambitious `ftarget`** for the global stage, as this can cause CMA-ES to run for its full `maxfevals` even if it's stuck in a suboptimal basin. Rely on `maxfevals` for time control, and let local refinement achieve the final precision.
  * For final polishing with gradient-free methods like **Nelder-Mead**, keep `maxiter` very low (e.g., **100-500 iterations**) if substantial refinement has already been done by gradient-based methods. Its purpose is to snap to the true non-smooth optimum, not to perform extensive search.
  * Custom objective functions handling min/max ratio computation. If using smooth approximations, ensure they are computationally efficient and only apply them for the final, precise local refinement stages, with a well-chosen `k` schedule.
- **Constraint handling**:
  * Penalty methods for boundary constraints
  * Projection operators to feasible region
  * Barrier methods for interior-point optimization
- **Advanced techniques (apply judiciously to manage `eval_time`)**:
  * Multi-start optimization: Focus on generating truly diverse and high-quality initializations (e.g., using quasi-random sequences or geometric heuristics) rather than simply increasing the number of random restarts.
  * Covariance Matrix Adaptation Evolution Strategy (CMA-ES)
  * Bayesian optimization (less common for this problem type but useful for extremely expensive objectives)

VALIDATION FRAMEWORK:
- **Geometric validation**:
  * Verify exactly 16 distinct points
  * Check coordinate bounds (typically [0,1] × [0,1])
  * Validate distance matrix symmetry and positivity
- **Data validation**:
  * All coordinates must be finite floats
  * No duplicate points (minimum separation threshold)
  * Proper handling of numerical precision issues
- **Solution quality assessment**:
  * Symmetry analysis of final configuration
  * Stability under small perturbations
  * Comparison with known geometric configurations
- **Optimization diagnostics**:
  * Convergence history tracking
  * Gradient norm analysis (when applicable)
  * Multi-run consistency checks
  
PROBLEM-SPECIFIC CONSIDERATIONS:
- **Initialization strategies**:
  * For global optimizers, especially multi-start or population-based ones, prioritize diverse and well-distributed initializations. **Quasi-random sequences (Sobol, Halton)** are highly effective for providing superior space coverage compared to purely random or simple geometric patterns, and should be the primary method for generating starting points for global search, or for the initial population of an evolutionary algorithm like CMA-ES.
  * Random uniform placement in unit square
  * Regular polygon vertices with small perturbations
  * Grid-based starting points with jitter
- **Objective function challenges**:
  * Non-smooth function (min/max operations): This necessitates robust global optimization and careful handling with local, gradient-based methods (e.g., smoothing, COBYLA).
  * Multiple local optima: Requires effective global search, but balance the exploration depth with `eval_time`. Avoid setting overly ambitious `ftarget` values in global optimizers if the problem is known to have many local optima, as this can lead to excessive runtime without finding a significantly better solution. Focus on achieving a *good* basin, then precisely refining it.
  * Sensitivity to small coordinate changes: Implies that the final local refinement should be very precise.
- **Trade-offs**: A critical aspect is the trade-off between achieving the absolute maximum `min_max_ratio` and maintaining a practical `eval_time`. The goal is to find an *optimal arrangement* efficiently. The `eval_time` constraint is paramount; a slightly lower ratio found quickly is preferred over a marginally better ratio taking orders of magnitude longer.
- **Constraint geometry**:
  * Unit square vs. unit circle domains
  * Periodic boundary conditions vs. hard boundaries
  * Allowable vs. required symmetries

# PROMPT-BLOCK-END
    
