# EVOLVE-BLOCK-START
import numpy as np 
from scipy.optimize import differential_evolution, minimize
import numba
from numpy.random import Generator, default_rng # For modern RNG
from scipy.stats.qmc import LatinHypercube # For diversified initial population

@numba.jit(nopython=True, cache=True)
def _numba_pdist_sq(points: np.ndarray) -> np.ndarray:
    """
    Numba-jitted function to compute squared pairwise distances efficiently.
    This is significantly faster than scipy.spatial.distance.pdist for this specific use case.
    """
    n_points_local = points.shape[0]
    num_distances = n_points_local * (n_points_local - 1) // 2
    distances_sq = np.empty(num_distances, dtype=points.dtype)
    k = 0
    for i in range(n_points_local):
        for j in range(i + 1, n_points_local):
            diff = points[i] - points[j]
            distances_sq[k] = np.dot(diff, diff)
            k += 1
    return distances_sq

def _objective(flat_coords: np.ndarray, n: int, d: int) -> float:
    """
    Objective function to minimize for differential_evolution and local refinement.
    It calculates -dmin/dmax using a fast, Numba-jitted distance calculation.

    Args:
        flat_coords (np.ndarray): A 1D array of flattened point coordinates (x1, y1, x2, y2, ...).
        n (int): The number of points.
        d (int): The dimension of the space.

    Returns:
        float: The negative of the minimum to maximum distance ratio.
               Returns np.inf if dmax is zero to penalize invalid configurations.
    """
    points = flat_coords.reshape((n, d))
    
    if n < 2: # Cannot compute distances for less than 2 points
        return np.inf

    distances_sq = _numba_pdist_sq(points)

    if len(distances_sq) == 0: # This should not happen if n >= 2, but for safety
        return np.inf

    dmin_sq = np.min(distances_sq)
    dmax_sq = np.max(distances_sq)

    # If dmax is zero, all points are identical. Penalize heavily.
    # Use a small threshold for floating point robustness.
    if dmax_sq < 1e-12:
        return np.inf
    
    # If dmin is zero, some points are coincident, but not all (dmax > 0). Ratio is 0.
    # Return 0.0 (negative of ratio is -0.0)
    if dmin_sq < 1e-12: # Check dmin_sq for robustness
        return 0.0

    # The ratio dmin/dmax is equivalent to sqrt(dmin_sq / dmax_sq).
    # This avoids taking the square root of the entire distance array, saving computation.
    ratio = np.sqrt(dmin_sq / dmax_sq)
    
    return -ratio

def _generate_hybrid_initial_population(
    n_points: int, dimensions: int, pop_size_multiplier: int, jitter_strength: float, 
    random_fraction: float, bounds: list, rng: Generator
) -> np.ndarray:
    """
    Generates a hybrid initial population for differential_evolution.
    A portion is based on jittered 4x4 grids, and the rest is from a Latin Hypercube sample
    to ensure broad diversity.
    Adapted from INSPIRATION programs and refined based on prompt guidelines.

    Args:
        n_points (int): The number of points (expected 16).
        dimensions (int): The dimensions (expected 2).
        pop_size_multiplier (int): Multiplier for `num_variables` to determine the total population size.
        jitter_strength (float): The maximum perturbation applied to grid points, relative to cell size.
        random_fraction (float): Fraction of the population to be generated by LHS.
        bounds (list): List of (min, max) tuples for each variable.
        rng (np.random.Generator): Random number generator for reproducibility.

    Returns:
        np.ndarray: An array of shape (pop_size, n_points * dimensions) representing the initial population.
    """
    if n_points != 16 or dimensions != 2:
        raise ValueError("This grid generation is specific to n=16, d=2 (4x4 grid).")

    num_variables = n_points * dimensions
    total_pop_size = pop_size_multiplier * num_variables
    
    num_random_lhs = int(total_pop_size * random_fraction)
    num_grid_jittered = total_pop_size - num_random_lhs
    
    initial_population = []
    
    grid_side = int(np.sqrt(n_points))
    min_coord, max_coord = bounds[0] # Assuming all variables have the same (0,1) bounds
    grid_cell_size = (max_coord - min_coord) / grid_side
    
    # Generate base grid points centered in each cell
    base_coords_1d = np.linspace(min_coord + grid_cell_size / 2, max_coord - grid_cell_size / 2, grid_side)
    base_grid_coords = np.array([[x, y] for y in base_coords_1d for x in base_coords_1d])

    # Generate jittered grid configurations
    for _ in range(num_grid_jittered):
        jitter = rng.uniform(-jitter_strength * grid_cell_size, jitter_strength * grid_cell_size, size=(n_points, dimensions))
        jittered_points = base_grid_coords + jitter
        jittered_points = np.clip(jittered_points, min_coord, max_coord)
        initial_population.append(jittered_points.flatten())

    # Generate Latin Hypercube Sample configurations for diversity
    if num_random_lhs > 0:
        # Pass the rng object to LatinHypercube for reproducibility and consistency
        sampler = LatinHypercube(d=num_variables, seed=rng) 
        random_population_lhs = sampler.random(n=num_random_lhs)
        # Scale LHS points from [0,1] to actual bounds (here, bounds are [0,1], so no scaling needed)
        initial_population.extend(random_population_lhs)
    
    # Add the unperturbed grid configuration as one of the initial candidates
    initial_population.append(base_grid_coords.flatten())

    return np.array(initial_population)

def min_max_dist_dim2_16() -> np.ndarray:
    """ 
    Creates 16 points in 2 dimensions in order to maximize the ratio of minimum to maximum distance.
    Uses a multi-stage optimization approach:
    1. Thorough global search with Differential Evolution (DE) and Numba-accelerated objective.
    2. Dedicated local refinement using the Powell method.
    """
    n = 16  # Number of points
    d = 2   # Dimensions

    # Define the search space bounds for each coordinate (0.0 to 1.0)
    bounds = [(0.0, 1.0)] * (n * d)
    seed_value = 42 # Fixed seed for reproducibility

    # Initialize the modern NumPy random number generator once
    rng = default_rng(seed_value)

    # --- Stage 1: Thorough Global Search with Differential Evolution ---
    # Generate initial population using a hybrid strategy: jittered grid + Latin Hypercube Sample.
    # This provides a diverse and geometrically informed starting set for DE.
    # pop_size_multiplier=20, num_variables=32 => total_pop_size = 640.
    # With random_fraction=0.5, ~320 jittered, ~320 LHS, plus 1 unperturbed grid.
    initial_population_de = _generate_hybrid_initial_population(
        n_points=n, 
        dimensions=d, 
        pop_size_multiplier=20, 
        jitter_strength=0.45, 
        random_fraction=0.5, # Use 50% LHS for strong diversity
        bounds=bounds, 
        rng=rng # Pass the modern RNG
    )

    # Run Differential Evolution with aggressively tuned parameters.
    # `init` parameter specifies the initial population, overriding `popsize`.
    de_result = differential_evolution(
        func=_objective,
        bounds=bounds,
        args=(n, d),
        strategy='randtobest1bin', # A robust strategy balancing exploration and exploitation.
        maxiter=30000,      # Further increased maxiter for even more thorough global search.
        # popsize is ignored when `init` is an array, so it is removed for clarity.
        mutation=(0.5, 1.0), # Standard mutation range is robust.
        recombination=0.9,   # High recombination favors features from better solutions.
        tol=1e-9,           # Even stricter tolerance for convergence.
        atol=1e-9,          # Even stricter absolute tolerance.
        disp=False,         # Set to True to display solver progress.
        polish=False,       # Disable DE's internal polish to use our dedicated local refinement.
        workers=-1,         # Use all available CPU cores for parallelization.
        updating='deferred',# Added for better parallel performance with multiple workers.
        seed=seed_value,    # Ensure reproducibility of DE's internal RNG.
        init=initial_population_de # Provide the custom, diverse initial population.
    )

    # --- Stage 2: Dedicated Local Refinement ---
    # Take the best solution found by Differential Evolution and apply a gradient-free
    # local optimizer for high-precision fine-tuning. 'Powell' is chosen for its
    # robustness with non-smooth functions and handling of bounds.
    
    # Define a helper objective for minimize, as it expects bounds slightly differently
    # (or handles them implicitly via Powell's method).
    def _local_objective_wrapper(flat_coords_local):
        return _objective(flat_coords_local, n, d)

    local_result = minimize(
        fun=_local_objective_wrapper,
        x0=de_result.x, # Start local search from the best DE solution
        method='Powell',
        bounds=bounds,
        options={'maxiter': 10000, 'fatol': 1e-10, 'xtol': 1e-10} # Further increased local iterations and stricter tolerances
    )

    # Compare results and select the best one.
    # If Powell finds a better (more negative) objective value, use its result.
    if local_result.fun < de_result.fun:
        optimized_params = local_result.x
    else:
        optimized_params = de_result.x # Fallback to DE's best if local search didn't improve

    optimized_points = optimized_params.reshape((n, d))

    return optimized_points
# EVOLVE-BLOCK-END