# EVOLVE-BLOCK-START
import numpy as np
from scipy.optimize import differential_evolution
import numba
from numpy.random import Generator, default_rng # New import for modern RNG
from scipy.stats.qmc import LatinHypercube # New import for Latin Hypercube Sampling

@numba.jit(nopython=True, cache=True)
def _numba_pdist_sq(points: np.ndarray) -> np.ndarray:
    n_points_local = points.shape[0]
    num_distances = n_points_local * (n_points_local - 1) // 2
    distances_sq = np.empty(num_distances, dtype=points.dtype)
    k = 0
    for i in range(n_points_local):
        for j in range(i + 1, n_points_local):
            diff = points[i] - points[j]
            distances_sq[k] = np.dot(diff, diff)
            k += 1
    return distances_sq

def _objective(flat_coords: np.ndarray, n: int, d: int) -> float:
    points = flat_coords.reshape((n, d))
    distances_sq = _numba_pdist_sq(points)

    if len(distances_sq) == 0:
        return np.inf

    dmin_sq = np.min(distances_sq)
    dmax_sq = np.max(distances_sq)

    if dmax_sq < 1e-12:
        return np.inf
    
    # Calculate ratio from squared distances to avoid a costly sqrt on the full array
    ratio = np.sqrt(dmin_sq / dmax_sq)
    return -ratio

def _generate_hybrid_initial_population(
    n_points: int, dimensions: int, pop_size: int, jitter_strength: float, random_fraction: float, rng: Generator
) -> np.ndarray:
    """
    Generates a hybrid initial population, combining a jittered grid with Latin Hypercube Sampling.
    This strategy provides both high-quality starting points and broad diversity. Inspired by Insp 1, 2, 3.

    Args:
        n_points (int): Number of points (must be a perfect square, e.g., 16).
        dimensions (int): Dimensionality of the space (e.g., 2).
        pop_size (int): The total desired size of the initial population.
        jitter_strength (float): The maximum perturbation applied to grid points, relative to cell size.
        random_fraction (float): Fraction of the population to be generated by Latin Hypercube Sampling.
        rng (np.random.Generator): Random number generator for reproducibility.
    """
    if int(np.sqrt(n_points))**2 != n_points:
        raise ValueError("Hybrid grid initialization requires a perfect square number of points (e.g., 16).")
    # For N=16, D=2, this implies a 4x4 grid.
    if n_points != 16 or dimensions != 2:
        raise ValueError("This grid generation is specific to n=16, d=2 (4x4 grid).")

    num_variables = n_points * dimensions
    num_lhs = int(pop_size * random_fraction)
    num_grid = pop_size - num_lhs
    
    initial_population = np.zeros((pop_size, num_variables))

    # Part 1: Jittered grid population for good starting points
    if num_grid > 0:
        grid_side = int(np.sqrt(n_points))
        cell_size = 1.0 / grid_side
        
        base_coords_1d = np.linspace(cell_size / 2, 1.0 - cell_size / 2, grid_side)
        grid_x, grid_y = np.meshgrid(base_coords_1d, base_coords_1d)
        base_points = np.vstack([grid_x.ravel(), grid_y.ravel()]).T

        for i in range(num_grid):
            # Jitter within the cell, then clip to [0,1]
            jitter = rng.uniform(-jitter_strength * cell_size, jitter_strength * cell_size, size=(n_points, dimensions))
            jittered_points = np.clip(base_points + jitter, 0.0, 1.0)
            initial_population[i, :] = jittered_points.flatten()

    # Part 2: Latin Hypercube Sample population for diversity
    if num_lhs > 0:
        # Use the same RNG for seeding LatinHypercube to ensure full reproducibility
        sampler = LatinHypercube(d=num_variables, seed=rng)
        lhs_population = sampler.random(n=num_lhs)
        initial_population[num_grid:, :] = lhs_population
        
    return initial_population

def min_max_dist_dim2_16() -> np.ndarray:
    """ 
    Creates 16 points in 2 dimensions using Differential Evolution to maximize the
    ratio of minimum to maximum distance. This approach is inspired by high-performing
    solutions that leverage DE's population-based search with custom, hybrid initialization.
    """
    n = 16  # Number of points
    d = 2   # Dimensions
    seed_value = 42

    bounds = [(0, 1)] * (n * d)
    
    # Use modern RNG for reproducibility across all stochastic components.
    rng = default_rng(seed_value)

    # Generate a hybrid initial population: part jittered grid, part Latin Hypercube Sample (LHS).
    # This combines good starting points with broad diversity for the optimizer, crucial for challenging problems.
    # A population size of 25 * (n * d) = 800 individuals is effective, based on inspirations.
    pop_size = 25 * (n * d) # Consistent with inspiration programs for a robust search
    initial_population = _generate_hybrid_initial_population(
        n_points=n, dimensions=d, pop_size=pop_size, 
        jitter_strength=0.4, # Used 0.4 from Insp 1, which had a slightly better ratio than 0.2 from Insp 2/3
        random_fraction=0.5, # 50% grid, 50% LHS for balanced exploration and exploitation
        rng=rng
    )

    # Run Differential Evolution with parameters tuned for a thorough, parallelized search
    result = differential_evolution(
        func=_objective,
        bounds=bounds,
        args=(n, d),
        strategy='randtobest1bin',  # Effective strategy for balancing exploration and exploitation.
        maxiter=15000,              # Increased maxiter from 12000 to 15000 for a more thorough global search,
                                    # aligning with the best performing inspiration (Insp 1 & 3).
        # popsize parameter is ignored when `init` is an ndarray, so its length determines the population size.
        mutation=(0.5, 1.0),        # Default mutation range - generally robust for DE.
        recombination=0.9,          # High recombination favors features from better solutions.
        tol=1e-8,                   # Strict tolerance to avoid premature convergence, maintained from target.
        atol=1e-8,                  # Strict absolute tolerance, maintained from target.
        disp=False,                 # Set to True to display solver progress.
        polish=True,                # Re-enabled: crucial for refining the best solution with L-BFGS-B,
                                    # as seen in the best performing inspirations (Insp 1 & 3).
        workers=-1,                 # Use all available CPU cores for parallelization.
        updating='deferred',        # Enabled for better parallel performance with workers > 1.
        seed=seed_value,            # Ensure reproducibility of DE's internal RNG.
        init=initial_population     # Provide the custom hybrid initial population.
    )

    optimized_points = result.x.reshape((n, d))
    return optimized_points
# EVOLVE-BLOCK-END