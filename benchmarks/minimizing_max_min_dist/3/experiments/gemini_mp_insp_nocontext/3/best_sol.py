# EVOLVE-BLOCK-START
import numpy as np
from scipy.optimize import dual_annealing, minimize
import numba
from scipy.stats import qmc # Re-added for Sobol sequence initialization

@numba.jit(nopython=True, fastmath=True)
def _pdist_numba(points: np.ndarray) -> np.ndarray:
    """
    Numba-accelerated implementation of pairwise Euclidean distances.
    This is essential for performance as the objective function is called millions of times.
    """
    n_points = points.shape[0]
    n_distances = n_points * (n_points - 1) // 2
    distances = np.empty(n_distances, dtype=np.float64)
    k = 0
    for i in range(n_points):
        for j in range(i + 1, n_points):
            dist_sq = 0.0
            for dim in range(points.shape[1]):
                diff = points[i, dim] - points[j, dim]
                dist_sq += diff * diff
            distances[k] = np.sqrt(dist_sq)
            k += 1
    return distances

@numba.jit(nopython=True, fastmath=True)
def _calculate_min_max_ratio_objective_numba(points_flat: np.ndarray, n_points: int, dimensions: int) -> float:
    """
    Numba-accelerated objective function for maximizing dmin/dmax (by minimizing -dmin/dmax).
    Returns a large penalty (np.inf) for degenerate configurations to guide the optimizer away.
    """
    points = points_flat.reshape((n_points, dimensions))
    
    if points.shape[0] < 2:
        return 0.0
    
    distances = _pdist_numba(points)
    
    if distances.shape[0] == 0:
        return 0.0

    dmin = np.min(distances)
    dmax = np.max(distances)
    
    if dmax < 1e-9: # Handle cases where all points are coincident or extremely close
        return np.inf 
            
    ratio = dmin / dmax
    return -ratio # For minimization algorithms
    
def min_max_dist_dim3_14()->np.ndarray:
    """ 
    Creates 14 points in 3 dimensions to maximize the ratio of minimum to maximum distance.
    This version implements an Iterated Hybrid Search (multi-start) strategy to escape
    strong local optima where previous single-run attempts got stuck.

    1.  **Multi-Start**: The entire optimization process is run multiple times from different,
        well-distributed starting points generated by a Sobol sequence.
    2.  **Hybrid Optimization**: Each run consists of a two-stage process:
        a. **Global Search (Dual Annealing)**: Broadly explores the search space.
        b. **Local Refinement (L-BFGS-B)**: Precisely converges the best global solution.
    3.  **Best-Result Tracking**: The best solution found across all runs is returned.

    This approach maximizes the chance of finding the true global optimum.
    """

    n = 14
    d = 3
    n_dims = n * d
    bounds = [(0, 1)] * n_dims
    optimization_seed = 42

    N_STARTS = 3  # Number of independent optimization runs
    
    # Initialize Sobol sampler to generate high-quality starting points for each run
    sampler = qmc.Sobol(d=n_dims, seed=optimization_seed)
    sampler.fast_forward(1)
    initial_guesses = sampler.random(n=N_STARTS)

    best_score = np.inf
    best_points_flat = None

    for i in range(N_STARTS):
        initial_x0 = initial_guesses[i]

        # --- Stage 1: Global Search with Dual Annealing ---
        # A more moderate budget is used for each run, reallocating effort to breadth (more starts)
        # rather than depth (one massive run).
        global_result = dual_annealing(
            func=_calculate_min_max_ratio_objective_numba,
            bounds=bounds,
            args=(n, d),
            x0=initial_x0,
            maxiter=5000,
            maxfun=20_000_000,
            seed=optimization_seed + i, # Vary seed slightly for each run's annealing path
            no_local_search=True
        )

        # --- Stage 2: Local Refinement with L-BFGS-B ---
        local_result = minimize(
            fun=_calculate_min_max_ratio_objective_numba,
            x0=global_result.x,
            args=(n, d),
            method='L-BFGS-B',
            bounds=bounds,
            options={'maxiter': 5000, 'ftol': 1e-12, 'gtol': 1e-9}
        )

        # Track the best result found across all runs
        if local_result.fun < best_score:
            best_score = local_result.fun
            best_points_flat = local_result.x

    optimal_points = best_points_flat.reshape((n, d))
    return optimal_points
# EVOLVE-BLOCK-END